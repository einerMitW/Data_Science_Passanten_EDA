---
title: "Weiner_Passanten01"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
bibliography: bib.bib
csl: ieee.csl
---

```{r setup, include=FALSE}
#Pdf Format Nachschlagewerk {r}:
#echo = FALSE /verhindert das der code Block im Pdf angezeigt wird.
#message=FALSE /Beim Knitten werden nun keine Meldungen oder Warnungen von diesem Chunk angezeigt.
#warning = FALSE /Warnungen wie Removed 3 rows containing missing values... von ggplot2 erscheinen nicht im PDF

# Globale Chunk-Optionen
library(knitr)

if (knitr::is_html_output()) {
  knitr::opts_chunk$set(
    dev       = "png",
    dpi       = 96,
    fig.retina= 2,
    fig.align = "center",
    fig.width = 8, fig.height = 5,
    out.width = "100%",
    message = FALSE, 
    warning = FALSE,
    crop = FALSE
  )
} else {
  knitr::opts_chunk$set(
    dev       = "cairo_pdf",
    fig.align = "center",
    fig.width = 7, fig.height = 4.5,
    message = FALSE, 
    warning = FALSE
  )
}

#Beispiel Doc String zur beschreibung aller der Funktionen
#' [Kurzer Titel der Funktion, z.B. Addiert zwei Zahlen]
#' @description
#' [Kurze Beschreibung, was die Funktion tut.]
#' @param param_name_1 [Beschreibung Parameter 1 (Typ, Zweck).]
#' @return
#' [Beschreibung des Rückgabewerts (Typ, Inhalt).]
```

```{r}
library(tidyverse)
library(readr)
library(broom)
library(gridExtra)
library(stats)
library(janitor)
library(skimr)
library(lubridate)
library(dplyr)
library(lubridate)
library(ggplot2)
library(fmsb)
library(corrplot)
```

```{r}
getwd()
#Globale Variablen
tage_vektor <- c("Montag", "Dienstag", "Mittwoch", "Donnerstag", "Freitag", "Samstag", "Sonntag")
monat_vektor <- c("Jan", "Feb", "Mar", "Apr", "Mai", "Jun", "Jul", "Aug", "Sep", "Okt", "Nov", "Dez" )
globale_farb_palette = c(
   "Kaiserstraße" = "#8D34D1",
   "Schönbornstraße" = "#D19534",
   "Spiegelstraße" = "#2EB865",
  
  # Die Aggregate
  "Gesamtdurchschnitt" = "#404040",
  "Gesamtsumme"        = "#404040"
)
```

# Passanten in Würzburg

## Import und Datenbereinigung

Am begin unserer Explorativen Datenanlyse (EDA) steht der Import der Rohdaten.

```{r}
passanten_raw <- read_csv2("Datensatz/passanten_wuerzburg.csv")
passanten <- as_tibble(passanten_raw)

passanten <- passanten %>%
  select(Zeitstempel,Wetter,Temperatur,Passanten,'Location Name',GeoPunkt) %>%
  mutate(
    jahr = year(Zeitstempel),
    monat = month(Zeitstempel),
    woche = isoweek(Zeitstempel),
    tag = day(Zeitstempel),
    stunde = hour(Zeitstempel),
    wochentag = weekdays(Zeitstempel),
    tag_im_jahr = yday(Zeitstempel)
  ) %>%
  filter(jahr != 2023)
```

Nun besitzt man die Daten als Dataframe "passanten" und zur refferenz als "passanten_raw". Dabei wurde der Dataframe passanten bereits in ein tibbel Format überführt (eine moderner Dataframe) und bereits angepasst. So wurden weitere spalten zum Dataframe hinzugefügt die den umgang mit den Daten später erleichtern z.B: ist nun jede woche über einen Index klar abfragbar. Auch wurden alle Daten aussortiert die nicht aus dem Jahr 2024 Stammen. Um nun eine sinvolle, descriptive oder weiterführend Explorative Datenanalyse durchzuführen sollten die Daten zuerst bereinigt werden. Als nächstes benennen wir die Spaltennamen in Snacke-Case um.

```{r}
passanten <- clean_names(passanten)
```

Anschließend wird die Anzahl an lehren fledern in den Spalten gezählt.

```{r}
kable(colSums(is.na(passanten)), digits = 0, caption = "Summe aller NA werte pro Spalte")
```

Es lässt sich entnehmen, das Wetter und temperatur jewails 112 NA einträge haben. Dies kann diverse gründe haben ist aber nicht relevant im moment. Nun weiß man, dass der Datensatz intakt ist und keine fehlenden Daten in wichtigen Spalten wie: zeitstempel und passanten vorliegen. Nun haben wir einen Datensatz mit dem man eine EDA durchführen kann.

Die folgende Beschreibung gibt bereits einen guten überblick über die Daten und wie sie erfasst werden.

## Aufgabe 1

## Beschreibung der Daten

Der Datensatz enthält daten zu einer Passantenzählung in der Nürnberger Innenstadt, aus dem Jahr 2024 Die wichtigste Spalte ist hierbei die Anzahl der Passanten welche in passanten gemessen wird. Dabei wird die Anzahl der Passanten mit jewails eine Zeitstempel und weiteren Metadaten wie: Temperatur, Ort der Aufzeichnung und Koordinaten des Ortes Zeilenweise angegeben. Man kann entnehmen, dass jede Stunde eine Aufzeichnung pro Location stattfindet.

```{r}
kable(skim(passanten), digits = 0, caption = "Skim überblick der Daten pro Spalte")
```

Der Datensatz besteht aus 26015 Zeile und 14 Spalten von denen 8 numerische, eine POSIXct und 4 Zeichenketten als Datentypen enthalten. Die Vollständigkeit ist bei allen auser den bereits beschriebenen Spalten gegeben. Ebenfalls ist zu erkennen, dass man mit 366 Tagen, 52 Wochen und 12 Monaten ein vollständig abgebildetes jahr hat. Auch zu erkennen ist das es sich durch die gesamtzahl von 366 Tagen um ein Schaltjahr handeln muss.

### Erhebung

Die Daten stamen von zählstationen an verschiedenen Punken aus der Nürnberger Innenstadt. Mit hilfe von Laserschranken zählen diese die Anzahl der Passanten welche gerade die Messtation Queren. Durch das Messen mit meheren Laserschranken pro Messtation kann auch die Geh-Richtung des Passanten bestimmt werden [@methodik]. Zur konsistenz der Daten wird vermerkt: "Nach Herstellerangabe kann mit der verwendeten Technik bis zu einem Durchfluss von ca. 500 Personen pro Minute eine Zählgenauigkeit von 99% erreicht werden." vgl. [@methodik]. Dabei ist zu beachten, dass eine Zählstation eine Straße bis Maximal 32m Breite abdecken kann. Die Erheber der daten versichern jedoch, dass "Bei den veröffentlichten Daten handelt es sich immer um die Passantenfrequenz der gesamten Straßenbreite (außer es ist explizit anders angegeben)." vgl. [@methodik].

### Standorte

```{r}
kable(table(passanten$location_name), digits = 0, caption = "Auflistung aller einzigartigen Location werte und wie of diese vorkommen")
kable(table(passanten$geo_punkt), digits = 0, caption = "Auflistung aller einzigartigen Geo Punkte und wie of diese vorkommen")
```

Wenn man angegebenen Koordinaten und Messtellen anschaut sieht man, dass es jewails drei unterschiedliche werte in diesen Spalten gibt. Entscheident ist nun die anzahl der dopplungen dieser werte. Bei genauerer betrachtung sieht man, das die jewailigen dopplungen der menge: location und Geo Punkt gleich sind. Damit kann man davon ausgehen, das jede Location einer eindeutigen Koordinate anhand der anzahl der Dopplungen zugeordnet werden kann. Wenn man nun die Punkte auf einer Karte einträgt erhält man folgende Übersicht:

```{r bild-einfuegen, echo=FALSE, out.width="50%", fig.cap="Markierte Standorte der Messtationen in der Nürnberger Innenstadt"}
# echo=FALSE versteckt den R-Code im finalen Dokument
# out.width="50%" setzt die Breite
# fig.cap="..." fügt eine Unterschrift hinzu

knitr::include_graphics("Bilder/wuerzburg_stadtplan_messtationen.png")
```

Die Koordinaten Stimmen mit der Jewailigen Straße und dem Name der Messtation überein. Man kann sehen, das die Stationen direkt in der Innenstad plaziert sind. Dabei ist jede Station in der Nähe einer Sehenswürdigkeit bzw. Öffentlichen Gebäude.

Die Messtation Schönbornstraße befindet sich nah an der Marienkapelle, die Messtation Spiegelstraße auf dem Weg zum Hofgarten und die Messtation Kaiserstraße ist in der nähe des Hauptbahnhof. Alle diese Straßen Hauptverkehrsstraßen auf denen mit vielen Passanten zu rechnen ist. Das Dreiecksmuster welche die Stationen Aufspannen bilden somit eine Art Transitstrecke zwischen: Hauptbahnhof -\> Hofgarten -\> Marienkapelle -\> Hauptbahnhof.

##Leitfragen
<!-- Eventuelles Aufstellen von Leitfragen Für die Analyse-->
Nach der Grundlegenden Beschreibung der Daten und einer einführung in den Datensatz sowie seinen Kontext folgen Leitfragen welche helfen die EDA zu leiten.
- Gibt es besondere Ereignisse in der Würzburger Innenstad welche erkennbar sind?
- Was ist die beliebteste Region in der Innenstad?
- Woher kommt die beliebtheit? (Attraktionen, Wichtiger Teil des Altags, Veranstaltungsorte, ...)

## Aufgabe 2

```{r}
#--- 1. Analyse aggregiert ---
# Funktion guppiert nach location_name und berechnet anschließend: passanten jahres Summe, erfasste Tage im Datensatz, durchschnitt pro Monat, durchsnitt pro Tag und durchschnitt pro Stunde.
# Eingabe: df passanten
# Ausgabe: aggregiert_jahressumme_pro_location
aggregiert_jahressume_pro_location <- passanten %>%
  group_by(location_name) %>%
  summarise(
    
    # 1. Jahressumme pro Standort
    passanten_jahr_summe = sum(passanten, na.rm = TRUE),
    
    anzahl_tage_erfasst = n_distinct(as.Date(zeitstempel)),
    
    # 2. Mittelwert pro Monat
    durchschnitt_pro_monat = passanten_jahr_summe / 12,
    
    # 3. Mittelwert pro Woche
    durchschnitt_pro_woche = passanten_jahr_summe / (anzahl_tage_erfasst / 7),

    # 4. Mittelwert pro Tag
    durchschnitt_pro_tag = passanten_jahr_summe / anzahl_tage_erfasst,
    
    # Mittelwert pro Stunde
    durchschnitt_pro_stunde = durchschnitt_pro_tag / 24
  )

# --- 2. Analyse aggregiert (insgesamt) ---
# Funktion berechnet die Jahressumme aller passenten um damit den Mittelwert über alle Stationen zu berechnen
# Eingabe: df passanten
# Ausgabe: passanten_insgesamt
passanten_insgesamt <- passanten %>%
  summarise(
    
    # 1. Jahressumme (Gesamt)
    passanten_jahr_summe = sum(passanten, na.rm = TRUE),
    
    # Hilfsberechnung: Anzahl der einzigartigen Tage im gesamten Datensatz
    anzahl_tage_erfasst = n_distinct(as.Date(zeitstempel)),

    # 2. Mittelwert pro Monat (Gesamt)
    durchschnitt_pro_monat = passanten_jahr_summe / 12,
    
    # 3. Mittelwert pro Woche (Gesamt)
    durchschnitt_pro_woche = passanten_jahr_summe / (anzahl_tage_erfasst / 7),
    
    # 4. Mittelwert pro Tag (Gesamt)
    durchschnitt_pro_tag = passanten_jahr_summe / anzahl_tage_erfasst,
    
    # 5. Mittelwert pro Stunde (Gesamt)
    durchschnitt_pro_stunde = durchschnitt_pro_tag / 24
  )

kable(aggregiert_jahressume_pro_location, digits = 0, caption = "Jahressumme und Mittelwerte der Passantenanzahl nach Messstelle")
kable(passanten_insgesamt, digits = 0, caption = "Jahressumme der Passantenanzahl")

```

Bereits durch die Einfache berechung der Mittelwerte pro Jahr, Monat, woche und Tag lässt sich festhalten, dass die Schönbornstraße die meiste Auslastung, mit fast 10 Millionen Passanten 2024 erhalten hat. Mit 21864318 Passanten die 2024 insgesammt gezählt wurden stellt die Schönbornstraße somit fast die Hälfte des gesamten Passanten verkehr. Den Daten nach ordnet sich die Kaiserstraße als zweit und die Spiegelstraße als am wenigsten ausgelastete Sraße an. Ebenfalls auffällig ist, dass die Schönbornstraße eine fast doppelt so hohe auslastung wie die Spiegelstraße hat. Somit ist hier eine ungeliche aufteilung der gesamtauslastung festzustellen.


Einfügen Interpretation der Lage und der Verbindungen die die Staßen aufspannen

## Aufgabe 3

```{r}
#' [Monatssumme über das Jahr]
#' @description
#' [Die Funktion Berechnet die Monatsumme der Passaten, gruppiert nach der location_name pro jahr]
#'
#' @param passanten df
#'
#' @return
#' [Rückgabewert ist ein der df sume_monat. Das wide Format zeigt die Daten sortiert nach monat und Location sowie gesamtsumme an]
#'
summe_monat <- passanten %>%
   group_by(location_name, monat) %>%
   summarise(monatssumme = sum(passanten)) %>%
   ungroup() %>%
   #Vertauschen der Zeilen und Spalten
   pivot_wider(names_from = location_name,
               values_from = monatssumme)%>%
   mutate(Gesamtsumme = rowSums(across(where(is.numeric)), na.rm = TRUE)) %>%
   arrange(monat)

kable(summe_monat, digits = 0, caption = "Summe aller Passanten pro Monat und Sraße")

#' [long plot format wandlung]
#' @description
#' [Formt den df summe_monat in ein long Format um]
#'
#' @param summe_monat 
#'
#' @return
#' [Gib den df summe_monat_plot in einem long Format aus welches zur Graphischen Darstellung verwendet werden kann]
mittelwert_monat_plot <- summe_monat %>% 
   pivot_longer(cols = -monat,
                names_to = "location_name",
                values_to = "gesamtsumme")
```
```{r}
plot_aufgabe3 <- ggplot(data = mittelwert_monat_plot, aes(x = monat, y = gesamtsumme, color = location_name)) +
  geom_line(linewidth = 1) +  # Zeichnet die Linien
  geom_point() +              # Fügt die Datenpunkte hinzu 
   scale_color_manual(values = globale_farb_palette)+
  scale_x_continuous(breaks = 1:12, labels = monat_vektor)+

  # Titel und Achsenbeschriftungen
  labs(title = "Summen pro Monat und Straße",
       x = "Monat",
       y = "Summe",
       color = "Standort")
plot_aufgabe3
```
Das Erstellte Diagramm bestätigt das aus Aufgabe2 bereits beschriebene Muster. Die Auslastung der Schönbornstraße ist konstant am höchsten gefolgt von der Kaiserstraße und der Spiegelstraße. Dise Reihenfolge ist über das Gesamte Jahr erkennbar was darauf schließen lässt das diese Auslastung nicht durch events oder ausergewöhnliche ereignisse, sondern eher den Alltag in der Nürnberger Innenstadt darstellt. 
In der Gesamtsummen kann man einen Anstieg ab April erkennen. Dies könnte auf den Beginn des Frühlings zurückzuführen sein.

Im September und Oktober scheinen einige Events die Anzahl der Passanten in der Innenstad erhöht zu habe. Das Liniendiagramm zeigt entsprechende ausreißer im September in der Gesamtsumme aber auch an den Jewailigen Stationen. Grund für die erhöhung von September auf Oktober sind mehrere Veranstaltungen. Zum einen das "Würzburger Stadfest" vom 14-15.Sep 2024 und die Würzburger Weinparade vom 29 August bis 8. September.Daraus ergibt sich er Anstig in der Gesamtsumme von ca 500.000 Passanten. Speziell auf der Kaiserstraße und Scönbornstraße zeigt das Liniendiagrammist ein markanter anstig der Gesamtzahl pro Messtation. Hierzu betrachten wir den Ort an dem das Stadfest stattfand.
Zum Stadfest ist zu sagen, das es eine dezentrale veranstaltung ist bei der "sich fast die gesamte Innenstadt in eine Vergnügungsmeile mit mehreren Bühnen, einem vielfältigen Programm" vgl. [@erlebenStadtfestWuerzburg20252025]. Mit dem Fehlenden Anstieg der Passantenzahl trifft die beschreibung "fast die gesamte Innenstad" sehr genau auf die tatsächlichen Daten zu. Somit trägt das Stadfest zur erhöhung der Passantenzahl bei ist aber nicht der grund für den erkennbaren anstieg auf der Kaiser und Schönborstraße.
Die Weinparade hingegen fand für einen Längeren Zeitraum exklusiv auf und um den Unteren markplatz bei der Marienkapelle, in direkter nachberschaft zur Schönbornstraße statt [@weinparadenwirtFest]. Da die Kaiserstraße eine direkte verbindung von Haupbahnhof und Marienkapelle darstellt lässt sich so der explizite ansieg der Passanten für diesen Zeitraum erklären.

Im Dezember gab es weitere veranstaltungen in der Innenstad. Hiebei zeigt das Liniendiagramm eine Anstieg in der Gesamtsumme aber auch auf der Schönborn und Spiegelstraße.Die Erklärung hierführ ist der Weihnachtsmarkt vom ende November bis zum 23.Dezember 2024[@krausIst2024Auf2024]. Veranstaltungsort ist hier ebenfalls der Obere und Untere Marktplatz und die Schönbornstraße, weshalb diese den höchsten anstieg verzeichnet.

## Aufgabe 4

```{r}
tagessumme <- passanten %>% 
   mutate(
      Datum = as_date(zeitstempel)
   ) %>%
   group_by(Datum, wochentag, location_name) %>%
   summarise(
    Tagessumme = sum(passanten, na.rm = TRUE)
    ) %>%
   ungroup()

gesamttagessumme <- passanten %>% 
   mutate(
      Datum = as_date(zeitstempel)
   ) %>%
   group_by(Datum, wochentag) %>%
   summarise(
    Tagessumme_Gesamt = sum(passanten, na.rm = TRUE), 
    .groups = 'drop' )


#' [durschnitt tageswert]
#' @description
#' [die Funktion gruppiert nach wochentag und location und berechnet dann den durchschnitt. gleichzeitig sortiert sie die wochentage in der korrenkten wöchentlichen anordungn]
#'
#' @param tagessumme
#' @return
#' [df durchnittlicher_tageswert der Passanten pro Messtation und Wochentag in long format]
durchschnittlicher_tageswert_location_plot <- tagessumme %>%
   mutate(
    wochentag = factor(wochentag, levels = tage_vektor)
   )%>%
   group_by(wochentag, location_name)%>%
   summarise(
      durchschnitt_wochentag_location = mean(Tagessumme)
   )%>%
   ungroup()


#' [durschnitt tageswert]
#' @description
#' [die Funktion gruppiert nach wochentag und location und berechnet dann den durchschnitt. gleichzeitig sortiert sie die wochentage in der korrenkten wöchentlichen anordungn]
#' @param tagessumme
#' @return
#' [df durchnittlicher_tageswert der Passanten pro Messtation und Wochentag in long format]
durchschnittlicher_tageswert_gesamt <- gesamttagessumme %>%
   mutate(
    wochentag = factor(wochentag, levels = tage_vektor)
   )%>%
   group_by(wochentag)%>%
   summarise(
      durchschnitt_wochentag = mean(Tagessumme_Gesamt)
   )%>%
   ungroup()

#' [df in Tabellenformat ]
#' @description
#' [Formatiert einen df in eine Aussagekräftige Tabelle]
#'
#' @param durschnittlicher_tageswert_plot]
#' @return
#' []
durchschnittlicher_tageswert_location_wide <- durchschnittlicher_tageswert_location_plot %>%
   pivot_wider(
      names_from = location_name,
      values_from = durchschnitt_wochentag_location
   ) %>% 
   left_join(durchschnittlicher_tageswert_gesamt,by = "wochentag")
kable(durchschnittlicher_tageswert_location_wide, digits = 0, caption = "Durschnittlicher Tageswert pro Location")
```

```{r}
#Forme durchschnittlicher_tageswert_gesamt um, sodass man ihn mit durchschnittlicher_tageswert_gesamt vereinen kann
durchschnittlicher_tageswert_gesamt_angepasst <- durchschnittlicher_tageswert_gesamt %>%
  rename(durchschnitt_wochentag_location = durchschnitt_wochentag) %>%
  mutate(location_name = "Gesamtdurchschnitt")

#Zusammenfügen der Beiden df's mit bind_rows
df_gesamt_long_plot <- bind_rows(
  durchschnittlicher_tageswert_location_plot,
  durchschnittlicher_tageswert_gesamt_angepasst
) %>%
  #Wochentage Richtig Sortieren 
  mutate(
    wochentag = factor(wochentag, levels = tage_vektor)
  )

plot_aufgabe4<- ggplot(
  data = df_gesamt_long_plot, 
  aes(x = wochentag, y = durchschnitt_wochentag_location, fill = location_name)
) +
   
  geom_col(position = "dodge") + 
  
  # Füge deine 4 Custom-Farben hinzu
  scale_fill_manual(values = globale_farb_palette) +
  
  labs(
    title = "Aufgabe 4: Durchschnittl. Tageswert (Gegliedert & Gesamt)",
    subtitle = "Als gruppiertes Balkendiagramm",
    x = "Wochentag",
    y = "Durchschnittlicher Tageswert",
    fill = "Messstelle"
  )
plot_aufgabe4

# kable(jahressumme, digits = 0,
#       caption = "Jahressumme und Mittelwerte der Passantenanzahl nach Messstelle")

```
Das Diagramm beschreibt die Durschnittliche Auslastung pro Tag, gegliedert nach dem Gesamtdurschnitt und den einzelnen Locations. Auch in der reingezoomten perspektive auf die Jahresdurschnitte auf den Wochentagen bleibt die Erkentniss aus Aufgabe 2 bestehen. Die Rangliste der der höchsten auslastung beginnt erneut mit der Schönbornstraße, gefolgt von der Kaiser und Spiegelstraße.

Die Graphik bestätigt den normalen Wochenrythmus. Sichtbar wird das an den Gelichbleiben Durchschnitten von Montag bis Donnerstag gefolgt von dem Anstieg an Freitag und Samstag und wenige Passanten am Sonntag. Ein Begründung für diesen Wochenrythmus ist Tägliche Geschäft in der Arbeitswoche gefolgt vom Wochenende an dem sich mehr Leute in der Innenstad aufhalten. Da am Sonntag viele Geschäfte geschlossen haben ist die Innenstad auch nicht so stark besucht. Bemerkenswert ist hier die gleichmäsige verteilung. Steigt der Gesamtdurchschnitt so steigt auch der durchschnitt jeder einzelenen Messtation. Das Spricht für eine gewisse Korrelation zwischen den Auslastungen der eizelnen Messtationen.

````{r}
#' [1. Datenaufbereitung für Korrelation (Aufgabe 4)]
#' @description
#' [Erstellt die "breite" Tabelle (7 Zeilen x 3 Spalten),
#' die für die Korrelationsanalyse benötigt wird.]
#' @param durchschnittlicher_tageswert_location_plot [DataFrame] Dein "langer" DF
#' @return
#' [DataFrame] 'wide_data_task4'
wide_data_task4 <- durchschnittlicher_tageswert_location_plot %>%
  pivot_wider(
    names_from = location_name,
    values_from = durchschnitt_wochentag_location
  )



#' [2. Korrelationsmatrix (Der "Beweis")]
#' @description
#' [Berechnet die Korrelationskoeffizienten zwischen den
#' 7-Tage-Mustern der Standorte.]
#' @param wide_data_task4 [DataFrame] Der breite DF von oben
correlation_data_task4 <- wide_data_task4 %>%
  select(Kaiserstraße, Schönbornstraße, Spiegelstraße)

cor_matrix_task4 <- cor(correlation_data_task4)

#' [3. Correlogramm (Der "visuelle Beweis")]
#' @description
#' [Visualisiert die Korrelationsmatrix als Heatmap.]
corrplot(cor_matrix_task4, type = "lower", method = "color")
````
Über die Berechnung der Korrelation erkennt man das klar den zusammenhang der Messtationen untereinander wobei Kaiser und Spieglstaße enger zusammenhängen als die jewailige straße mit der Schönbornestraße. Das habe die Schönbornstraße weiter hervor wenn auch nicht sehr stark.

## Aufgabe 5

```{r}
#'[Stündl. Mittelwert (Gegliedert)]
#' @description
#' Berechnet den durchschnittlichen Passantenwert für jede Stunde (0-23)
#' und für jede einzelne Messstelle ('location_name').
#' @param passanten [DataFrame] Das Roh-DataFrame 'passanten'.
#'   Benötigt die Spalten 'stunde', 'location_name' und 'passanten'.
#' @return
#' [DataFrame] 'passantenanzahl_stunden_plot' (langes Format).
#'   Enthält 72 Zeilen (24h * 3 Messstellen) mit dem stündlichen Mittelwert.
passantenanzahl_stunden_plot <- passanten %>%
   group_by(stunde, location_name) %>%
   summarise(
      passantenanzahl = mean(passanten)
      )%>%
   ungroup()



#' [Stündl. Mittelwert (Summiert)]
#' @description
#' Berechnet den durchschnittlichen Passantenwert für jede Stunde (0-23)
#' über ALLE Messstellen hinweg (Gesamtdurchschnitt).
#' @param passanten [DataFrame] Das Roh-DataFrame 'passanten'.
#'   Benötigt die Spalten 'stunde' und 'passanten'.
#' @return
#' [DataFrame] 'avg_stunde_gesamt' (langes Format).
#'   Enthält 24 Zeilen (eine pro Stunde) mit dem Gesamt-Stundenmittelwert.
avg_stunde_gesamt <- passanten %>%
   group_by(stunde) %>%
   summarise(
      durchschnitt_passanten_gesamt = mean(passanten, na.rm = TRUE)
   )%>%
   ungroup()


#' [Stündl. Mittelwert (Breites Format)]
#' @description
#' Wandelt das "lange" Plot-Format in ein "breites" Tabellen-Format um.
#' Jede Messstelle wird zu einer eigenen Spalte.
#' @param passantenanzahl_stunden_plot [DataFrame] Das "lange" Ergebnis aus Block 1.
#' @return
#' [DataFrame] 'passantenanzahl_stunden_wide'.
#'   Enthält 24 Zeilen (eine pro Stunde) und Spalten für jede Messstelle.   
passantenanzahl_stunden_wide <- passantenanzahl_stunden_plot %>%
   pivot_wider(
      names_from = location_name,
      values_from = passantenanzahl
      )%>%
   left_join(avg_stunde_gesamt, by = "stunde")
passantenanzahl_stunden_wide

```

```{r}

ggplot() +
   # 1. Plot
  geom_line(data = passantenanzahl_stunden_plot, 
            aes(x = stunde, y = passantenanzahl, 
            color = location_name),
            linewidth = 1) +
  geom_point(data = passantenanzahl_stunden_plot, 
             aes(x = stunde, y = passantenanzahl, 
            color = location_name)) +

   # 2. Plot
  geom_line(
    data = avg_stunde_gesamt, 
    aes(x = stunde, y = durchschnitt_passanten_gesamt, colour = NULL, group = 1),
    color = "black", # <-- DIESE ANWEISUNG HINZUFÜGEN
    linewidth = 1.5,
    linetype = "dashed"
  ) + 
   
   scale_color_manual(values = globale_farb_palette)+
   
   scale_x_continuous(breaks = seq(0, 23, by = 2))
   
  labs(title = "Aufgabe 5: Durchschnittlicher Tagesverlauf (Gegliedert & Gesamt)",
       x = "Uhrzeit (Stunde des Tages)",
       y = "Durchschnittliche Passanten pro Stunde",
       color = "Standort")
```

### Extra Plot

```{r}
#' [Aufgabe 5: Heatmap des Tagesverlaufs]
#' @description
#' [Zeigt die Passantenanzahl als farbige Kacheln.
#' Ideal, um "Hot Spots" (Peaks) schnell zu identifizieren.]
library(ggplot2)
# library(viridis) # Für eine farbenblinde-sichere Palette (optional)

ggplot(passantenanzahl_stunden_plot, 
       aes(x = stunde, y = location_name, fill = passantenanzahl)) +
  
  geom_tile(color = "white") + # 'color="white"' fügt dünne weiße Ränder hinzu
  
  # Wir brauchen eine sequentielle Farbskala (kontinuierlich)
  scale_fill_viridis_c(option = "C") + # "viridis_c" ist eine gute Standard-Palette
  # Oder: scale_fill_gradient(low = "lightblue", high = "darkblue")
  
  scale_x_continuous(breaks = seq(0, 23, by = 2)) +
  
  labs(
    title = "Aufgabe 5: Heatmap des Tagesverlaufs",
    x = "Uhrzeit (Stunde des Tages)",
    y = "Messstelle",
    fill = "Durchschnittl.\nPassanten" # \n für Zeilenumbruch
  ) +
  theme_minimal()
```

```{r}
#' [Aufgabe 5: (Einzel-) Flächendiagramm für Gesamtsumme]
#' @description
#' [Zeigt den Gesamtdurchschnitt über den Tag als gefüllte Fläche.
#' Betont das Gesamtvolumen.]

ggplot(avg_stunde_gesamt, 
       aes(x = stunde, y = durchschnitt_passanten_gesamt)) +
  
  # Erst die Fläche (mit Transparenz)
  geom_area(fill = "steelblue", alpha = 0.5) +
  
  # Dann die Linie (im selben Farbton, aber dunkler/solide)
  geom_line(color = "steelblue", linewidth = 1.5) +
  
  scale_x_continuous(breaks = seq(0, 23, by = 2)) +
  
  labs(
    title = "Aufgabe 5: Gesamter Tagesverlauf (als Area Chart)",
    x = "Uhrzeit (Stunde des Tages)",
    y = "Durchschnittliche Passanten (Alle Standorte)"
  ) +
  theme_minimal()
```

```{r}
#' [3. Multiple Lineare Regression]
#' @description
#' [Modelliert einen Standort (Kaiserstraße) als Funktion
#' der BEIDEN anderen Standorte, um deren gemeinsamen
#' und individuellen Einfluss zu quantifizieren.]
#' @param passantenanzahl_stunden_wide [DataFrame] Dein "breiter" DF.
#'   Benötigt die Spalten 'Kaiserstraße', 'Schönbornstraße', 'Spiegelstraße'.
#' @return
#' [lm-Objekt] 'model' enthält das trainierte Regressionsmodell.
#' [Text-Output] 'summary(model)' zeigt die Ergebnisse (wie in deinem Screenshot).

correlation_data <- passantenanzahl_stunden_wide %>%
  select(Kaiserstraße, Schönbornstraße, Spiegelstraße)

model <- lm(Kaiserstraße ~ Schönbornstraße + Spiegelstraße, data = correlation_data)

summary(model)



#' [1. "Intuitiver" Modell-Plot (Dumbbell-Plot)]
#' @description
#' [Visualisiert die Güte des Modells, indem TATSÄCHLICHE Werte
#' (aus der Tabelle) direkt den VORHERGESAGTEN Werten (aus dem Modell)
#' für jede Stunde gegenübergestellt werden.]
#' @param model [lm-Objekt] Dein Regressionsmodell ('model')
#' @param passantenanzahl_stunden_wide [DataFrame] Dein "breiter" DF,
#'   der 'stunde' und die Prädiktoren (Schönbornstraße etc.) enthält.

model_data_augmented <- augment(model, newdata = passantenanzahl_stunden_wide)

ggplot(model_data_augmented, aes(x = stunde)) +
  
  geom_point(
    aes(y = Kaiserstraße, color = "Tatsächlich"), # y = Deine echte Spalte
    size = 3
  ) +
  
  geom_point(
    aes(y = .fitted, color = "Vorhergesagt"), # y = Die Modell-Prognose
    size = 3, 
    shape = 4 # 'shape = 4' ist ein 'X'
  ) +
  
  geom_segment(
    aes(xend = stunde, y = Kaiserstraße, yend = .fitted), 
    color = "grey", 
    linewidth = 0.7
  ) +
  
  # --- 4. Manuelle Farb- und Legendensteuerung ---
  scale_color_manual(
    name = "Wert-Typ",
    values = c("Tatsächlich" = "blue", "Vorhergesagt" = "red")
  ) +
  
  scale_x_continuous(breaks = seq(0, 23, by = 2)) +
  
  labs(
    title = "Modell-Visualisierung: Tatsächliche vs. Vorhergesagte Werte",
    subtitle = "Die gestrichelte Linie zeigt den Modellfehler (Residual) pro Stunde",
    x = "Stunde (0-23)",
    y = "Passanten durchnitt je Stunde (Kaiserstraße)"
  ) +
  theme_minimal()
```
````{r}
library(lubridate) # Falls noch nicht geladen

#' [Monat extrahieren]
#' @description
#' [Stellt sicher, dass die 'monat'-Spalte (numerisch 1-12)
#' für die Aggregation verfügbar ist.]
passanten_mit_monat <- passanten %>%
  mutate(
    # (Stelle sicher, dass 'zeitstempel_dt' existiert, 
    # siehe Code von Aufgabe 4/5 'Datum extrahieren')
    monat = month(zeitstempel, label = FALSE) 
  )

#' [Stündl. Mittelwert pro Monat (Gegliedert)]
#' @description
#' [Berechnet den durchschnittlichen Passantenwert PRO STUNDE
#' für jeden Monat (1-12) und jede Messstelle.
#' (Logik von A5, angewandt auf A3)]
avg_hourly_by_month_plot <- passanten_mit_monat %>%
   group_by(monat, location_name) %>%
   summarise(
      # Das ist die Logik aus Aufgabe 5: mean(passanten)
      durchschnitt_pro_stunde = mean(passanten, na.rm = TRUE)
      ) %>%
   ungroup()

#' [Plot: Jahresverlauf der stündl. Mittelwerte]
#' @description
#' [Zeigt den Jahresverlauf der durchschnittlichen stündlichen 
#' Auslastung pro Standort.]
ggplot(avg_hourly_by_month_plot, 
       aes(x = monat, y = durchschnitt_pro_stunde, color = location_name)) +
  
  geom_line(linewidth = 1.2) +
  geom_point(size = 2) +
  
  # Verwende deine globale Palette
  scale_color_manual(values = globale_farb_palette) +
  
  # Zeige alle 12 Monate auf der Achse
  scale_x_continuous(breaks = 1:12) +
  
  labs(
    title = "Jahresverlauf: Durchschnittliche Stündliche Auslastung",
    subtitle = "Zeigt den 'typischen' Stundenwert pro Monat",
    x = "Monat (1-12)",
    y = "Durchschnittliche Passanten pro Stunde",
    color = "Standort"
  ) +
  theme_minimal()
````
````{r}

````


## Fazit


# Literatur
