---
title: "Weiner_Passanten01"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
  html_notebook:
    toc: true
bibliography: bib.bib
csl: ieee.csl
---

```{r setup, include=FALSE}
#Pdf Format Nachschlagewerk {r}:
#echo = FALSE /verhindert, dass der code Block im Pdf angezeigt wird.
#message=FALSE /Beim Knitten werden nun keine Meldungen oder Warnungen von diesem Chunk angezeigt.
#warning = FALSE /Warnungen wie Removed 3 rows containing missing values... von ggplot2 erscheinen nicht im PDF

# Globale Chunk-Optionen
library(knitr)

if (knitr::is_html_output()) {
  knitr::opts_chunk$set(
    dev       = "png",
    dpi       = 96,
    fig.retina= 2,
    fig.align = "center",
    fig.width = 8, fig.height = 5,
    out.width = "100%",
    message = FALSE, 
    warning = FALSE,
    crop = FALSE
  )
} else {
  knitr::opts_chunk$set(
    dev       = "cairo_pdf",
    fig.align = "center",
    fig.width = 7, fig.height = 4.5,
    message = FALSE, 
    warning = FALSE
  )
}

#Beispiel Doc String zur beschreibung aller der Funktionen
#' [Kurzer Titel der Funktion, z.B. Addiert zwei Zahlen]
#' @description
#' [Kurze Beschreibung, was die Funktion tut.]
#' @param param_name_1 [Beschreibung Parameter 1 (Typ, Zweck).]
#' @return
#' [Beschreibung des Rückgabewerts (Typ, Inhalt).]
```

```{r}
library(tidyverse)
library(readr)
library(broom)
library(gridExtra)
library(stats)
library(janitor)
library(skimr)
library(lubridate)
library(dplyr)
library(lubridate)
library(ggplot2)
library(fmsb)
library(corrplot)
```

```{r}
getwd()
#Globale Variablen
tage_vektor <- c("Montag", "Dienstag", "Mittwoch", "Donnerstag", "Freitag", "Samstag", "Sonntag")
monat_vektor <- c("Jan", "Feb", "Mar", "Apr", "Mai", "Jun", "Jul", "Aug", "Sep", "Okt", "Nov", "Dez" )
globale_farb_palette = c(
   "Kaiserstraße" = "#8D34D1",
   "Schönbornstraße" = "#D19534",
   "Spiegelstraße" = "#2EB865",
  
  # Die Aggregate
  "Gesamtdurchschnitt" = "#404040",
  "Gesamtsumme"        = "#404040"
)
```

# Passanten in Würzburg

## Einleitung

Lebendige Innenstädte sind das Zentrum, die Einkaufsmeile und der Treffpunkt einer Stadt. Um diese Ströme zu verstehen, führt diese Arbeit eine explorative Datenanalyse (EDA) eines offenen Datensatzes der Stadt Würzburg durch. Dabei werden die stündlichen Passantenzahlen an drei zentralen Messstellen (Kaiserstraße, Schönbornstraße und Spiegelstraße) im Jahr 2024 untersucht. Ziel ist es, die zentralen Muster und Rhythmen der Passantennutzung zu identifizieren. Mittels des Interpretationsprogramm R werden die zeitlichen Entwicklungen im Jahres-, Wochen- und Tagesverlauf sowie die Auslastung der Standorte verglichen.

## Import und Datenbereinigung

Am Beginn unserer Explorativen Datenanalyse (EDA) steht der Import der Rohdaten.

```{r}
passanten_raw <- read_csv2("Datensatz/passanten_wuerzburg.csv")
passanten <- as_tibble(passanten_raw)

passanten <- passanten %>%
  select(Zeitstempel,Wetter,Temperatur,Passanten,'Location Name',GeoPunkt) %>%
  mutate(
    jahr = year(Zeitstempel),
    monat = month(Zeitstempel),
    woche = isoweek(Zeitstempel),
    tag = day(Zeitstempel),
    stunde = hour(Zeitstempel),
    wochentag = weekdays(Zeitstempel),
    tag_im_jahr = yday(Zeitstempel)
  ) %>%
  filter(jahr == 2024)
```

Damit besitzt man die Daten als Dataframe „passanten“ und zur Referenz als „passanten_raw“. Der Dataframe passanten wurde bereits in ein Tibble-Format überführt (ein moderner Dataframe) und angepasst. So wurden weitere Spalten zum Dataframe hinzugefügt, die den Umgang mit den Daten später erleichtern, z. B. ist nun jede Woche über einen Index klar abfragbar. Auch wurden alle Daten aussortiert, die nicht aus dem Jahr 2024 stammen. Um nun eine sinnvolle, deskriptive oder weiterführende Explorative Datenanalyse durchzuführen, sollten die Daten zuerst bereinigt werden. Als Nächstes benennen wir die Spaltennamen in Snake-Case um.

```{r}
passanten <- clean_names(passanten)
```

Anschließend wird die Anzahl an leeren Feldern in den Spalten gezählt.

```{r}
kable(colSums(is.na(passanten)), digits = 0, caption = "Summe aller NA-Werte pro Spalte")
```

Es lässt sich entnehmen, dass Wetter und Temperatur jeweils 112 NA-Einträge haben. Dies kann diverse Gründe haben, wie z. B. Reparaturen, Defekte oder Stromausfälle. Daten in wichtigen Spalten wie `zeitstempel` und `passanten` (welche wir für diese Analyse hauptsächlich verwenden werden) sind nicht von NA-Werten betroffen. Nun weiß man, dass der Datensatz für die Betrachtung dieser Werte intakt ist. Mit diesem aufbereiteten Datensatz kann man nun eine EDA durchführen.

## 1. Datenbasis und Methodik

Die folgende Beschreibung gibt bereits einen guten Überblick über die Daten und wie sie erfasst werden.

### Beschreibung des Datensatz

Der Datensatz enthält Daten zu einer Passantenzählung in der Würzburger Innenstadt aus dem Jahr 2024. Die wichtigste Spalte ist hierbei die Anzahl der Passanten, welche in `passanten` gemessen wird. Dabei wird die Anzahl der Passanten mit jeweils einem Zeitstempel und weiteren Metadaten wie Temperatur, Ort der Aufzeichnung und Koordinaten des Ortes zeilenweise angegeben. Man kann entnehmen, dass jede Stunde eine Aufzeichnung pro Location stattfindet.

```{r}
skim(passanten)
```

Der Datensatz besteht aus 26015 Zeilen und 13 Spalten, von denen 8 numerische, eine POSIXct und 4 Zeichenketten als Datentypen enthalten. Die Vollständigkeit ist bei allen außer den bereits beschriebenen Spalten gegeben. Ebenfalls ist zu erkennen, dass man mit 366 Tagen, 52 Wochen und 12 Monaten ein vollständig abgebildetes Jahr hat. Auch zu erkennen ist, dass es sich durch die Gesamtzahl von 366 Tagen um ein Schaltjahr handeln muss.

### Erhebung

Die Daten stammen von Zählstationen an verschiedenen Punkten aus der Würzburger Innenstadt. Mit Hilfe von Laserschranken zählen diese die Anzahl der Passanten, welche gerade die Messstation queren. Durch das Messen mit mehreren Laserschranken pro Messstation kann auch die Geh-Richtung des Passanten bestimmt werden [@methodik]. Zur Konsistenz der Daten wird vermerkt: „Nach Herstellerangabe kann mit der verwendeten Technik bis zu einem Durchfluss von ca. 500 Personen pro Minute eine Zählgenauigkeit von 99% erreicht werden“, vgl. [@methodik]. Dabei ist zu beachten, dass eine Zählstation eine Straße bis maximal 32m Breite abdecken kann. Die Erheber der Daten versichern: „Bei den veröffentlichten Daten handelt es sich immer um die Passantenfrequenz der gesamten Straßenbreite (außer es ist explizit anders angegeben)“, vgl. [@methodik].

### Standorte

```{r}
kable(table(passanten$location_name), digits = 0, caption = "Auflistung aller einzigartigen Standorte")
kable(table(passanten$geo_punkt), digits = 0, caption = "Auflistung aller einzigartigen Geo Punkte")
```

Wenn man die angegebenen Koordinaten und Messstellen anschaut, sieht man, dass es jeweils drei unterschiedliche Werte in diesen Spalten gibt. Entscheidend ist nun die Anzahl der Dopplungen dieser Werte. Bei genauerer Betrachtung sieht man, dass die jeweiligen Dopplungen der Menge `location` und `Geo Punkt` gleich sind. Damit kann man davon ausgehen, dass jede Location einer eindeutigen Koordinate anhand der Anzahl der Dopplungen zugeordnet werden kann. Wenn man nun die Punkte auf einer Karte einträgt, erhält man folgende Übersicht:

```{r bild-einfuegen, echo=FALSE, out.width="50%", fig.cap="Markierte Standorte der Messstationen in der Würzburger Innenstadt"}
# echo=FALSE versteckt den R-Code im finalen Dokument
# out.width="50%" setzt die Breite
# fig.cap="..." fügt eine Unterschrift hinzu

knitr::include_graphics("Bilder/wuerzburg_stadtplan_messtationen.png")
```

Die Koordinaten stimmen mit der jeweiligen Straße und dem Namen der Messstation überein. In der Abbildung sieht man, dass die Stationen direkt in der Innenstadt platziert sind. Dabei ist jede Station in der Nähe einer Sehenswürdigkeit bzw. eines öffentlichen Gebäudes.

Die Messstation Schönbornstraße befindet sich nah an der Marienkapelle, die Messstation Spiegelstraße auf dem Weg zum Hofgarten und die Messstation Kaiserstraße ist in der Nähe des Hauptbahnhofs. Alle diese Straßen sind Hauptverkehrsstraßen, auf denen mit vielen Passanten zu rechnen ist. Das Dreiecksmuster, welches die Stationen aufspannen, bildet somit eine Art Transitstrecke zwischen: Hauptbahnhof -\> Hofgarten -\> Marienkapelle -\> Hauptbahnhof.

### Leitfragen

Nach der grundlegenden Beschreibung der Daten und einer Einführung in den Datensatz sowie seinen Kontext folgen Leitfragen, welche helfen, die EDA zu leiten.

1.  Was ist die beliebteste Region in der Innenstadt?
2.  Woher kommt die Beliebtheit (Attraktionen, wichtiger Teil des Alltags, Veranstaltungsorte, ...)?
3.  Gibt es besondere Ereignisse in der Würzburger Innenstadt, welche erkennbar sind?

## 2. Aggregierte Jahresanalyse der Standorte

```{r}
#--- 1. Analyse aggregiert ---
# Funktion guppiert nach location_name und berechnet anschließend: passanten jahres Summe, erfasste Tage im Datensatz, durchschnitt pro Monat, durchsnitt pro Tag und durchschnitt pro Stunde.
# Eingabe: df passanten
# Ausgabe: aggregiert_jahressumme_pro_location
aggregiert_jahressume_pro_location <- passanten %>%
  group_by(location_name) %>%
  summarise(
    
    # 1. Jahressumme pro Standort
    passanten_jahr_summe = sum(passanten, na.rm = TRUE),
    
    anzahl_tage_erfasst = n_distinct(as.Date(zeitstempel)),
    
    # 2. Mittelwert pro Monat
    durchschnitt_pro_monat = passanten_jahr_summe / 12,
    
    # 3. Mittelwert pro Woche
    durchschnitt_pro_woche = passanten_jahr_summe / (anzahl_tage_erfasst / 7),

    # 4. Mittelwert pro Tag
    durchschnitt_pro_tag = passanten_jahr_summe / anzahl_tage_erfasst,
    
    # Mittelwert pro Stunde
    durchschnitt_pro_stunde = durchschnitt_pro_tag / 24
  )

# --- 2. Analyse aggregiert (insgesamt) ---
# Funktion berechnet die Jahressumme aller passenten um damit den Mittelwert über alle Stationen zu berechnen
# Eingabe: df passanten
# Ausgabe: passanten_insgesamt
passanten_insgesamt <- passanten %>%
  summarise(
    
    # 1. Jahressumme (Gesamt)
    passanten_jahr_summe = sum(passanten, na.rm = TRUE),
    
    # Hilfsberechnung: Anzahl der einzigartigen Tage im gesamten Datensatz
    anzahl_tage_erfasst = n_distinct(as.Date(zeitstempel)),

    # 2. Mittelwert pro Monat (Gesamt)
    durchschnitt_pro_monat = passanten_jahr_summe / 12,
    
    # 3. Mittelwert pro Woche (Gesamt)
    durchschnitt_pro_woche = passanten_jahr_summe / (anzahl_tage_erfasst / 7),
    
    # 4. Mittelwert pro Tag (Gesamt)
    durchschnitt_pro_tag = passanten_jahr_summe / anzahl_tage_erfasst,
    
    # 5. Mittelwert pro Stunde (Gesamt)
    durchschnitt_pro_stunde = durchschnitt_pro_tag / 24
  )

tabelle_standorte_sauber <- aggregiert_jahressume_pro_location %>%
  rename(
    "Standort" = location_name,
    "Summe (Jahr)" = passanten_jahr_summe,
    "Tage (erfasst)" = anzahl_tage_erfasst,
    "Schnitt (Monat)" = durchschnitt_pro_monat,
    "Schnitt (Woche)" = durchschnitt_pro_woche,
    "Schnitt (Tag)" = durchschnitt_pro_tag,
    "Schnitt (Stunde)" = durchschnitt_pro_stunde
  )

tabelle_gesamt_sauber <- passanten_insgesamt %>%
  rename(
    "Summe (Jahr)" = passanten_jahr_summe,
    "Tage (erfasst)" = anzahl_tage_erfasst,
    "Schnitt (Monat)" = durchschnitt_pro_monat,
    "Schnitt (Woche)" = durchschnitt_pro_woche,
    "Schnitt (Tag)" = durchschnitt_pro_tag,
    "Schnitt (Stunde)" = durchschnitt_pro_stunde
  )

kable(tabelle_standorte_sauber, digits = 0, caption = "Jahressumme und Mittelwerte der Passantenanzahl nach Messstelle")
kable(tabelle_gesamt_sauber, digits = 0, caption = "Jahressumme der Passantenanzahl")
```

Bereits durch die einfache Berechnung der Mittelwerte pro Jahr, Monat, Woche und Tag lässt sich festhalten, dass die Schönbornstraße die höchste Auslastung, mit fast 10 Millionen Passanten 2024, erhalten hat. Mit 21864318 Passanten, die 2024 insgesamt gezählt wurden, stellt die Schönbornstraße somit 43% des gesamten Passantenverkehrs dar. Den Daten nach ordnet sich die Kaiserstraße als zweite und die Spiegelstraße als am wenigsten genutzte Straße an. Ebenfalls auffällig ist, dass die Schönbornstraße eine fast doppelt so hohe Auslastung wie die Spiegelstraße hat. Somit ist hier eine ungleiche Aufteilung der Gesamtauslastung festzustellen.

## 3. Analyse des Saisonalen Jahresverlaufs

Hier wird die zeitliche Entwicklung der Passantenfrequenzen über das Jahr als Monatssumme visualisiert. Ziel ist die Identifikation und Interpretation saisonaler Muster, wie etwa Einflüsse durch Jahreszeiten oder Feiertage.

```{r}
#' [Monatssumme über das Jahr]
#' @description
#' [Die Funktion berechnet die Monatsumme der Passaten, gruppiert nach der location_name pro jahr]
#'
#' @param passanten df
#'
#' @return
#' [Rückgabewert ist der Dataframe summe_monat. Das Wide-Format zeigt die Daten sortiert nach Monat und Location sowie Gesamtsumme an]
#'
summe_monat <- passanten %>%
   group_by(location_name, monat) %>%
   summarise(monatssumme = sum(passanten)) %>%
   ungroup() %>%
   #Vertauschen der Zeilen und Spalten
   pivot_wider(names_from = location_name,
               values_from = monatssumme)%>% 
   mutate(Gesamtsumme = rowSums(across(where(is.numeric)), na.rm = TRUE)) %>%
   arrange(monat)

kable(summe_monat, digits = 0, caption = "Summe aller Passanten nach Monat und pro Straße wie über alle Straßen summiert")

#' [Umwandlung in Long-Format für den Plot]
#' @description
#' [Formt den Dataframe summe_monat in ein Long-Format um]
#'
#' @param summe_monat 
#'
#' @return
#' [Gibt den Dataframe summe_monat_plot in einem Long-Format aus, welches zur grafischen Darstellung verwendet werden kann]
mittelwert_monat_plot <- summe_monat %>%
   pivot_longer(cols = -monat,
                names_to = "location_name",
                values_to = "gesamtsumme")
```

```{r}
plot_aufgabe3 <- ggplot(data = mittelwert_monat_plot, aes(x = monat, y = gesamtsumme, color = location_name)) +
  geom_line(linewidth = 1) +  # Zeichnet die Linien
  geom_point() +              # Fügt die Datenpunkte hinzu 
   scale_color_manual(values = globale_farb_palette)+
  scale_x_continuous(breaks = 1:12, labels = monat_vektor)+

  # Titel und Achsenbeschriftungen
  labs(title = "Summen nach Monat und pro Straße",
       subtitle = "Als Liniendiagramm",
       x = "Monat",
       y = "Passantenanzahl",
       color = "Standort")
plot_aufgabe3
```

Das erstellte Diagramm bestätigt das aus Aufgabe 2 bereits beschriebene Muster. Die Auslastung der Schönbornstraße ist konstant am höchsten, gefolgt von der Kaiserstraße und der Spiegelstraße. Diese Reihenfolge ist über das gesamte Jahr erkennbar, was darauf schließen lässt, dass diese Auslastung nicht durch Events oder außergewöhnliche Ereignisse, sondern eher den Alltag in der Würzburger Innenstadt darstellt. In der Gesamtsumme kann man einen Anstieg ab April erkennen. Dies könnte auf den Beginn des Frühlings zurückzuführen sein.

Im Oktober und Dezember scheinen einige Events die Anzahl der Passanten in der Innenstadt erhöht zu haben. Das Liniendiagramm zeigt entsprechende Ausreißer im Oktober in der Gesamtsumme, aber auch an den jeweiligen Stationen. Grund für die Erhöhung im Oktober könnten mehrere Veranstaltungen im Oktober sein. Die größte davon ist die Allerheiligenmesse, ein mehrtägiger Markt in der Innenstadt historischen Ursprungs mit verkaufsoffenem Sonntag (Mantelsonntag am 27.10.2024) [@WuerzburgMachtSpass]. Der Mantelsonntag ist ein jährliches Ereignis am letzten Sonntag im Oktober vgl [@WuerzburgMachtSpass].„Am Einkaufstag für die ganze Familie haben die Geschäfte in der Würzburger Innenstadt von 13 bis 18 Uhr geöffnet. Viele kostenfreie Parkplätze an der Oberfläche und Tiefpreisparken in den Parkgaragen machen das Einkaufen am einzigen verkaufsoffenen Sonntag Würzburgs zum Vergnügen.“ [@WuerzburgMachtSpass]. Dieses Ereignis erklärt nicht allein den Anstieg in der Gesamtsumme von ca. 240.000 Passanten, dennoch liefert es einen Beitrag dazu. Speziell auf der Kaiserstraße und Schönbornstraße zeigt das Liniendiagramm einen markanten Anstieg der Gesamtzahl pro Messstation. Hierzu betrachten wir die Orte, auf die der verkaufsoffene Mantelsonntag einen Einfluss hat. An der Schönbornstraße, einer der Haupteinkaufsstraßen in Würzburg vgl[@StadtBUMMELWurzburgWurzburgs], erklärt sich die erhöhte Nutzung für einen verkaufsoffenen Sonntag. Wie man aus der oben gezeigten Abbildung entnehmen kann, ist die Kaiserstraße eine direkte Verbindung vom Hauptbahnhof zur Schönbornstraße. Somit dient sie als Transitstraße für die An- und Abreise über den Hauptbahnhof. Zusätzlich zählt sie ebenfalls zu den Einkaufsstraßen. So lässt sich auch der Anstieg in der Kaiserstraße erklären.

Im Dezember gab es weitere Veranstaltungen in der Innenstadt. Hierbei zeigt das Liniendiagramm einen Anstieg in der Gesamtsumme, aber auch auf der Schönborn- und Spiegelstraße. Die Erklärung hierfür ist der Weihnachtsmarkt von Ende November bis zum 23. Dezember 2024 [@krausIst2024Auf2024]. Veranstaltungsort ist hier ebenfalls der Obere und Untere Marktplatz und die Schönbornstraße, weshalb diese den höchsten Anstieg verzeichnet.

## 4. Analyse des Wöchentlichen Rhythmus

Diese Sektion analysiert den typischen Wochenrhythmus, indem der durchschnittliche Tageswert pro Wochentag untersucht wird. Dadurch werden die charakteristischen Unterschiede zwischen Werktagen und dem Wochenende herausgearbeitet und interpretiert.

```{r}
tagessumme <- passanten %>%
   mutate(
      Datum = as_date(zeitstempel)
   ) %>%
   group_by(Datum, wochentag, location_name) %>%
   summarise(
    Tagessumme = sum(passanten, na.rm = TRUE)
    ) %>%
   ungroup()

gesamttagessumme <- passanten %>%
   mutate(
      Datum = as_date(zeitstempel)
   ) %>%
   group_by(Datum, wochentag) %>%
   summarise(
    Tagessumme_Gesamt = sum(passanten, na.rm = TRUE), 
    .groups = 'drop' )


#' [Mittlerer Tageswert (Gegliedert, für Plot)]
#' @description
#' Berechnet den durchschnittlichen TAGESwert (mean(Tagessumme))
#' für jeden Wochentag, aufgeschlüsselt nach 'location_name'.
#' Sortiert zudem die Wochentage (als Faktor) in die korrekte Reihenfolge.
#' @param tagessumme [DataFrame] Der DF 'tagessumme' (Ergebnis von Block 1).
#' @return
#' [DataFrame] 'durchschnittlicher_tageswert_location_plot' (langes Format).
#'   Enthält den mittleren Tageswert pro Wochentag und Location (21 Zeilen).
durchschnittlicher_tageswert_location_plot <- tagessumme %>%
   mutate(
    wochentag = factor(wochentag, levels = tage_vektor)
   ) %>%
   group_by(wochentag, location_name)%>% 
   summarise(
      durchschnitt_wochentag_location = mean(Tagessumme)
   )%>% 
   ungroup()


#' [Mittlerer Tageswert (Gesamt)]
#' @description
#' Berechnet den durchschnittlichen GESAMT-Tageswert (mean(Tagessumme_Gesamt))
#' für jeden Wochentag (summiert über alle Standorte).
#' Sortiert zudem die Wochentage (als Faktor) in die korrekte Reihenfolge.
#' @param gesamttagessumme [DataFrame] Der DF 'gesamttagessumme' (Ergebnis von Block 2).
#' @return
#' [DataFrame] 'durchschnittlicher_tageswert_gesamt' (langes Format).
#'   Enthält den mittleren Gesamt-Tageswert pro Wochentag (7 Zeilen).
durchschnittlicher_tageswert_gesamt <- gesamttagessumme %>%
   mutate(
    wochentag = factor(wochentag, levels = tage_vektor)
   ) %>%
   group_by(wochentag)%>% 
   summarise(
      durchschnitt_wochentag = mean(Tagessumme_Gesamt)
   )%>% 
   ungroup()

#' [Kombinierte Tabelle (Breites Format)]
#' @description
#' Kombiniert die gegliederten und die gesamten Mittelwerte in einer "breiten" Tabelle.
#' Diese Tabelle ist ideal für die tabellarische Darstellung in der Analyse.
#' @param durchschnittlicher_tageswert_location_plot [DataFrame] Der DF aus Block 3.
#' @param durchschnittlicher_tageswert_gesamt [DataFrame] Der DF aus Block 4.
#' @return
#' [DataFrame] 'durchschnittlicher_tageswert_location_wide'.
#'   Enthält 7 Zeilen (pro Wochentag) und Spalten für jeden Standort sowie die Gesamtsumme.
durchschnittlicher_tageswert_location_wide <- durchschnittlicher_tageswert_location_plot %>%
   pivot_wider(
      names_from = location_name,
      values_from = durchschnitt_wochentag_location
   ) %>%
   left_join(durchschnittlicher_tageswert_gesamt,by = "wochentag")
kable(durchschnittlicher_tageswert_location_wide, digits = 0, caption = "Durchschnittlicher Tageswert pro Straße")
```

```{r}
#Forme durchschnittlicher_tageswert_gesamt um, sodass man ihn mit durchschnittlicher_tageswert_gesamt vereinen kann
durchschnittlicher_tageswert_gesamt_angepasst <- durchschnittlicher_tageswert_gesamt %>%
  rename(durchschnitt_wochentag_location = durchschnitt_wochentag) %>%
  mutate(location_name = "Gesamtdurchschnitt")

#Zusammenfügen der Beiden df's mit bind_rows
df_gesamt_long_plot <- bind_rows(
  durchschnittlicher_tageswert_location_plot,
  durchschnittlicher_tageswert_gesamt_angepasst
) %>%
  #Wochentage Richtig Sortieren 
  mutate(
    wochentag = factor(wochentag, levels = tage_vektor)
  )

plot_aufgabe4<- ggplot(
  data = df_gesamt_long_plot, 
  aes(x = wochentag, y = durchschnitt_wochentag_location, fill = location_name)
) + 
   
  geom_col(position = "dodge") + 
  
  # Füge deine 4 Custom-Farben hinzu
  scale_fill_manual(values = globale_farb_palette) + 
  
  labs(
    title = "Durchschnittlicher Tageswert (Gegliedert nach Straßen & Gesamt)",
    subtitle = "Als gruppiertes Balkendiagramm",
    x = "Wochentag",
    y = "Durchschnittlicher Passantenanzahl",
    fill = "Messstelle"
  )
plot_aufgabe4

# kable(jahressumme, digits = 0,
#       caption = "Jahressumme und Mittelwerte der Passantenanzahl nach Messstelle")

```

Das Diagramm beschreibt die durchschnittliche Auslastung pro Tag, gegliedert nach dem Gesamtdurchschnitt und den einzelnen Locations. Auch in der reingezoomten Perspektive auf die Jahresdurchschnitte auf den Wochentagen bleibt die Erkenntnis aus Aufgabe 2 bestehen. Die Rangliste der höchsten Auslastung beginnt erneut mit der Schönbornstraße, gefolgt von der Kaiser- und Spiegelstraße.

Die Grafik bestätigt den normalen Wochenrhythmus. Sichtbar wird das an den gleichbleibenden Durchschnitten von Montag bis Donnerstag, gefolgt von dem Anstieg am Freitag und Samstag und wenigen Passanten am Sonntag. Eine Begründung für diesen Wochenrhythmus ist das tägliche Geschäft in der Arbeitswoche, gefolgt von Freitagabend und Samstag, an dem sich mehr Leute in der Innenstadt aufhalten. Da am Sonntag viele Geschäfte geschlossen haben, ist die Innenstadt auch nicht so stark besucht. Bemerkenswert ist hier die gleichmäßige Verteilung am Sonntag zwischen den einzelnen Straßen. Das gibt Aufschluss über die Grundauslastung der einzelnen Straßen ohne den Einfluss von Arbeitstagen oder offenen Geschäften. Dennoch bleibt die bereits erkannte Rangfolge unter den Straßen bestehen. Ebenfalls auffällig ist die Korrelation unter den Straßen. Steigt der Gesamtdurchschnitt, so steigt auch der Durchschnitt jeder einzelnen Messstation. Das spricht für eine gewisse Korrelation zwischen den Auslastungen der einzelnen Straßen.

```{r}
#' [1. Datenaufbereitung für Korrelation (Aufgabe 4)]
#' @description
#' [Erstellt die "breite" Tabelle (7 Zeilen x 3 Spalten),
#' die für die Korrelationsanalyse benötigt wird.]
#' @param durchschnittlicher_tageswert_location_plot [DataFrame] Dein "langer" DF
#' @return
#' [DataFrame] 'wide_data_task4'
wide_data_task4 <- durchschnittlicher_tageswert_location_plot %>%
  pivot_wider(
    names_from = location_name,
    values_from = durchschnitt_wochentag_location
  )

#' [2. Korrelationsmatrix (Der "Beweis")]
#' @description
#' [Berechnet die Korrelationskoeffizienten zwischen den
#' 7-Tage-Mustern der Standorte.]
#' @param wide_data_task4 [DataFrame] Der breite DF von oben
correlation_data_task4 <- wide_data_task4 %>%
  select(Kaiserstraße, Schönbornstraße, Spiegelstraße)

cor_matrix_task4 <- cor(correlation_data_task4)

#' [3. Correlogramm (Der "visuelle Beweis")]
#' @description
#' [Visualisiert die Korrelationsmatrix als Heatmap.]
corrplot(cor_matrix_task4, type = "lower", method = "color")
```

Über die Berechnung der Korrelation erkennt man eine starke Korrelation. Ein Wert \>0.5 gilt als starker Zusammenhang. Aus der Korrelationsmatrix lässt sich eine Korrelation \>0.5 für alle Straßen ablesen. Somit kann man davon ausgehen, dass die Auslastung der einzelnen Straßen sich untereinander beeinflusst bzw. einem Rhythmus folgen, der alle Standorte betrifft.

## 5. Analyse des täglichen Profils

Auf der granularsten Ebene wird das durchschnittliche Tagesprofil (0-23 Uhr) analysiert. Diese Analyse zielt darauf ab, tageszeitliche Spitzen (z.B. Rush-Hour) und spezifische Nutzungstypen der Standorte zu identifizieren.

```{r}
#'[Stündl. Mittelwert (Gegliedert)]
#' @description
#' Berechnet den durchschnittlichen Passantenwert für jede Stunde (0-23)
#' und für jede einzelne Messstelle ('location_name').
#' @param passanten [DataFrame] Das Roh-DataFrame 'passanten'.
#'   Benötigt die Spalten 'stunde', 'location_name' und 'passanten'.
#' @return
#' [DataFrame] 'passantenanzahl_stunden_plot' (langes Format).
#'   Enthält 72 Zeilen (24h * 3 Messstellen) mit dem stündlichen Mittelwert.
passantenanzahl_stunden_plot <- passanten %>%
   group_by(stunde, location_name) %>%
   summarise(
      passantenanzahl = mean(passanten)
      )%>% 
   ungroup()



#' [Stündl. Mittelwert (Summiert)]
#' @description
#' Berechnet den durchschnittlichen Passantenwert für jede Stunde (0-23)
#' über ALLE Messstellen hinweg (Gesamtdurchschnitt).
#' @param passanten [DataFrame] Das Roh-DataFrame 'passanten'.
#'   Benötigt die Spalten 'stunde' und 'passanten'.
#' @return
#' [DataFrame] 'avg_stunde_gesamt' (langes Format).
#'   Enthält 24 Zeilen (eine pro Stunde) mit dem Gesamt-Stundenmittelwert.
avg_stunde_gesamt <- passanten %>%
   group_by(stunde) %>%
   summarise(
      durchschnitt_passanten_gesamt = mean(passanten, na.rm = TRUE)
   )%>% 
   ungroup()


#' [Stündl. Mittelwert (Breites Format)]
#' @description
#' Wandelt das "lange" Plot-Format in ein "breites" Tabellen-Format um.
#' Jede Messstelle wird zu einer eigenen Spalte.
#' @param passantenanzahl_stunden_plot [DataFrame] Das "lange" Ergebnis aus Block 1.
#' @return
#' [DataFrame] 'passantenanzahl_stunden_wide'.
#'   Enthält 24 Zeilen (eine pro Stunde) und Spalten für jede Messstelle.   
passantenanzahl_stunden_wide <- passantenanzahl_stunden_plot %>%
   pivot_wider(
      names_from = location_name,
      values_from = passantenanzahl
      )%>% 
   left_join(avg_stunde_gesamt, by = "stunde")
kable(passantenanzahl_stunden_wide, digits = 0, caption = "Durchschnittlicher Passantenwert jede Stunde (0-23)")
```

```{r}

ggplot() +
   # 1. Plot
  geom_line(data = passantenanzahl_stunden_plot, 
            aes(x = stunde, y = passantenanzahl, 
            color = location_name),
            linewidth = 1) + 
  geom_point(data = passantenanzahl_stunden_plot, 
             aes(x = stunde, y = passantenanzahl, 
            color = location_name)) + 

   # 2. Plot
  geom_line(
    data = avg_stunde_gesamt, 
    aes(x = stunde, y = durchschnitt_passanten_gesamt, colour = NULL, group = 1),
    color = "black",
    linewidth = 1.5,
    linetype = "dashed"
  ) + 
   
   scale_color_manual(values = globale_farb_palette)+
   
   scale_x_continuous(breaks = seq(0, 23, by = 2))
   
  labs(title = "Durchschnittlicher Tagesverlauf (Gegliedert nach Straße & Gesamt)",
       subtitle = "Als Liniendiagramm",
       x = "Uhrzeit (Stunde des Tages)",
       y = "Durchschnittliche Passanten pro Stunde",
       color = "Standort")
```

Das auf Grundlage der Tabelle erstellte Liniendiagramm zeigt den Durchschnitt der Passanten pro Stunde, aufgeteilt nach Straßen. Die gestrichelte Linie stellt dabei den Gesamtdurchschnitt über alle Straßen pro Stunde dar.

Die bereits festgestellte Rangliste nach Auslastung bestätigt sich auch in dieser Ansicht.

Morgens pendeln mehr Leute über die Kaiserstraße. Das ist im Bereich zwischen 4 und 7 Uhr auf dem Graphen abzulesen. Die Kaiserstraße liegt dabei über dem Gesamtdurchschnitt, der Schönborn- und Spiegelstraße. Eine Begründung ist die Lage der Kaiserstraße. Sie ist die direkte Verbindung vom Hauptbahnhof in die Innenstadt. Da die Anzahl der Werktage die Anzahl der Feiertage + gesetzlich vorgegebener Urlaubstage übertrifft, kann man davon ausgehen, dass der Durchschnitt stark durch den morgendlichen Verkehr zum Hauptbahnhof beeinflusst ist.

Die Kaiserstraße scheint demnach den Gesamtdurchschnitt sehr genau abzubilden. Ablesbar ist dies aus dem Diagramm, da die Kaiserstraße und der Gesamtdurchschnitt einen ähnlichen Verlauf haben.

```{r}
#' [Aufgabe 5: Heatmap des Tagesverlaufs]
#' @description
#' [Zeigt die Passantenanzahl als farbige Kacheln.
#' Ideal, um "Hot Spots" (Peaks) schnell zu identifizieren.]
library(ggplot2)
# library(viridis) # Für eine farbenblinde-sichere Palette (optional)

ggplot(passantenanzahl_stunden_plot, 
       aes(x = stunde, y = location_name, fill = passantenanzahl)) + 
  
  geom_tile(color = "white") + # 'color="white"' fügt dünne weiße Ränder hinzu
  
  # Wir brauchen eine sequentielle Farbskala (kontinuierlich)
  scale_fill_viridis_c(option = "C") + # "viridis_c" ist eine gute Standard-Palette
  # Oder: scale_fill_gradient(low = "lightblue", high = "darkblue")
  
  scale_x_continuous(breaks = seq(0, 23, by = 2)) + 
  
  labs(
    title = "Heatmap des Tagesverlaufs",
    x = "Uhrzeit (Stunde des Tages)",
    y = "Messstelle",
    fill = "Durchschnittl.
Passanten"
  ) +
  theme_minimal()
```

Die Heatmap dient als effektive, komplementäre Visualisierung der stündlichen Passantenfrequenzen. Durch die Darstellung der Passantenanzahl mittels Farbintensität ermöglicht sie eine schnelle Identifizierung von "Hot Spots" und bestätigt die bereits im Liniendiagramm gewonnenen Erkenntnisse. Dies zeigt sich beispielsweise in der deutlich intensiveren Färbung der Kaiserstraße in den frühen Morgenstunden sowie den hellsten Bereichen für alle Standorte am Nachmittag, was die zuvor beschriebenen Muster visuell unterstreicht. \## Vorhersagemodell

Durch die in Abschnitt 4 beschriebene Korrelation zwischen den Straßen in der durchschnittlichen Tageswert stellt sich nun die Frage, ob man die Auslastung einer Straße durch die Werte der beiden anderen vorhersagen kann. Aus Abschnitt 5 wissen wir, dass die Kaiserstraße sehr ähnlich zur durchschnittlichen Gesamtauslastung ist. Daher wird versucht, die Kaiserstraße durch die Spiegel- und Schönbornstraße vorherzusagen.

```{r}
#' [3. Multiple Lineare Regression]
#' @description
#' [Modelliert einen Standort (Kaiserstraße) als Funktion
#' der BEIDEN anderen Standorte, um deren gemeinsamen
#' und individuellen Einfluss zu quantifizieren.]
#' @param passantenanzahl_stunden_wide [DataFrame] Dein "breiter" DF.
#'   Benötigt die Spalten 'Kaiserstraße', 'Schönbornstraße', 'Spiegelstraße'.
#' @return
#' [lm-Objekt] 'model' enthält das trainierte Regressionsmodell.
#' [Text-Output] 'summary(model)' zeigt die Ergebnisse (wie in deinem Screenshot).

correlation_data <- passantenanzahl_stunden_wide %>%
  select(Kaiserstraße, Schönbornstraße, Spiegelstraße)

model <- lm(Kaiserstraße ~ Schönbornstraße + Spiegelstraße, data = correlation_data)
zusammenfassung <- summary(model)
zusammenfassung

#kable(zusammenfassung, caption= "Zusammenfassung der Regressionsberechnung.")

```

Die erstellte lineare Regression liefert ein Modell, um die Passantenfrequenz der Kaiserstraße auf Basis der Frequenzen der Schönbornstraße und Spiegelstraße vorherzusagen. Die Zusammenfassung des Modells (`summary(model)`) gibt Aufschluss über die Güte und die Zusammenhänge:

**Statistische Signifikanz p-value:** Die erste und wichtigste Zeile, welche man für eine Auswertung betrachtet, ist die unterste: F-statistic: 2382 on 2 and 21 DF, p-value: \< 2.2e-16. Das p-value ist hier mit 2.2e-16 angegeben. Diese sehr kleinen p-Werte deuten darauf hin, dass der Einfluss beider Straßen auf die Kaiserstraße hoch signifikant ist. Es ist also äußerst unwahrscheinlich, dass dieser starke Zusammenhang nur ein Zufallsprodukt in den Daten ist.

**Modellgüte (`Adjusted R-squared`):** Das adjustierte R-Quadrat (`Adjusted R-squared`) ist das wichtigste Maß für die Güte des Modells. Der Wert von 0.9956 bedeutet, dass das Modell **99 %** der Varianz (der Schwankungen) in den Passantenzahlen der Kaiserstraße allein durch die Daten der beiden anderen Straßen erklären kann. Ein derart hoher Wert zeigt eine exzellente Passform und bestätigt, dass die Passantenströme der drei Straßen systemisch stark miteinander verknüpft sind. Die Güte des berechneten Modells ist hier sehr hoch.

**Interpretation der Koeffizienten (`Coefficients`):** Die Koeffizienten zeigen, wie stark die beiden anderen Straßen die Kaiserstraße beeinflussen. Der positive Koeffizient von 0.21 für die Schönbornstraße liefert für das Modell folgende Berechnung: Ein Passant mehr in der Schönbornstraße führt zu 0.2 Passanten mehr in der Kaiserstraße.

```{r}


#' [1. "Intuitiver" Modell-Plot (Dumbbell-Plot)]
#' @description
#' [Visualisiert die Güte des Modells, indem TATSÄCHLICHE Werte
#' (aus der Tabelle) direkt den VORHERGESAGTEN Werten (aus dem Modell)
#' für jede Stunde gegenübergestellt werden.]
#' @param model [lm-Objekt] Dein Regressionsmodell ('model')
#' @param passantenanzahl_stunden_wide [DataFrame] Dein "breiter" DF,
#'   der 'stunde' und die Prädiktoren (Schönbornstraße etc.) enthält.
model_data_augmented <- augment(model, newdata = passantenanzahl_stunden_wide)

ggplot(model_data_augmented, aes(x = stunde)) + 
  
  geom_point(
    aes(y = Kaiserstraße, color = "Tatsächlich"), # y = Deine echte Spalte
    size = 3
  ) + 
  
  geom_point(
    aes(y = .fitted, color = "Vorhergesagt"), # y = Die Modell-Prognose
    size = 3, 
    shape = 4 # 'shape = 4' ist ein 'X'
  ) + 
  
  geom_segment(
    aes(xend = stunde, y = Kaiserstraße, yend = .fitted), 
    color = "green", 
    linewidth = 1
  ) + 
  
  # --- 4. Manuelle Farb- und Legendensteuerung ---
  scale_color_manual(
    name = "Wert-Typ",
    values = c("Tatsächlich" = "blue", "Vorhergesagt" = "red")
  ) + 
  
  scale_x_continuous(breaks = seq(0, 23, by = 2)) + 
  
  labs(
    title = "Modell-Visualisierung: Tatsächliche vs. Vorhergesagte Werte",
    x = "Stunde (0-23)",
    y = "Passantendurchschnitt je Stunde (Kaiserstraße)"
  ) + 
  theme_minimal()
```

Das Diagramm stellt die tatsächlichen Werte (blaue Punkte) den vom Modell vorhergesagten Werten (rote Kreuze) für jede Stunde gegenüber. - Die grünen Verbindungslinien visualisieren den Vorhersagefehler (die Residuen) für jede Stunde. - Man erkennt, dass die Linien insgesamt sehr kurz sind, was die hohe Genauigkeit des Modells visuell bestätigt. Die Vorhersagen liegen sehr nah an den echten Werten.

Eine Abweichung, bei der das Modell weniger Passantenvorhersagt (Tatsächlicher Wert liegt über der Vorhersage) ist morgens (5 - 7 Uhr) und nachmittags im Zeitraum 15-18 Uhr. Diese Abweichung könnte durch das erhöhte Pendleraufkommen am Hauptbahnhof zu Beginn und Ende eines Arbeitstages entstehen, welche jedoch nicht die beiden anderen Straßen passieren.

Im Zeitraum von 10 bis 13 Uhr sagt das Modell einen höheren Wert voraus. Das bedeutet, dass bei Annahme der Auslastung von Schönborn- und Spiegelstraße die Kaiserstraße voller sein müsste. Aus dem Liniendiagramm der Aufgabe 5 ist zu erkennen, dass die Spiegelstraße um 10 Uhr einen „Mittagspeak“ hat. Die Kaiserstraße hingegen stagniert um 10 Uhr, bevor sie gegen 15 Uhr ihren Peak erreicht. Der Peak der Spiegelstraße könnte hier die Erklärung für das „Überschätzen“ des Modells sein. Der Grund hierfür könnte die Benutzung der Spiegelstraße in der Mittagspause sein, z. B. in Richtung Hofgarten, während jedoch die Pendlerverkehre an der Kaiserstraße im Verhältnis geringer sind.

Zusammenfassend lässt sich sagen, dass das lineare Regressionsmodell hervorragend geeignet ist, die Passantenfrequenz der Kaiserstraße vorherzusagen. Dies untermauert die bereits in der Korrelationsanalyse gezeigte starke systemische Verbindung der Passantenströme an den drei betrachteten Pukten der Innenstadt.

## Fazit

Die Explorative Datenanalyse (EDA) der Würzburger Passantenzahlen an drei Punkten in der Innenstadt für das Jahr 2024 hat erfolgreich grundlegende Muster und Zusammenhänge aufgedeckt. Als Antwort auf die Leitfragen wurde die Schönbornstraße eindeutig als belebtester Standort identifiziert, deren hohe Frequenz auf ihre zentrale Lage als Einkaufsstraße und die Nähe zu Sehenswürdigkeiten zurückzuführen ist. Die Analyse bestätigte klare Wochen- und Tagesrhythmen, die dem typischen Innenstadtleben mit Pendler- und Einkaufsverkehr entsprechen. Zudem konnten saisonale Ereignisse wie der Weihnachtsmarkt und die Allerheiligenmesse mit Mantelsonntag als deutliche Ausreißer in den Daten nachgewiesen werden.

Die starke Korrelation und das erfolgreiche Regressionsmodell untermauern, dass die drei Messstationen als vernetztes System agieren. Ein Ausbau der Analyse könnte fortführend noch die Wetterdaten berücksichtigen. Zukünftige Analysen könnten daher untersuchen, welchen quantitativen Einfluss Temperatur und Wetter auf die Passantenfrequenz haben, um die Vorhersagemodelle weiter zu verfeinern und ein noch umfassenderes Bild der urbanen Mobilität zu zeichnen.

# Literatur
